{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"+1\">\n",
        "<font color='red'>\n",
        "<b> IMPORTANT NOTE: </b> \n",
        "</font>\n",
        "Make sure to save a copy of this notebook in your personal drive to maintain the changes you make!\n",
        "</font>"
      ],
      "metadata": {
        "id": "Z3VsQ5VNdvXp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp7nb9C7KLkC"
      },
      "source": [
        "Run the following cells to prepare your working environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv1y8gFvKLkF"
      },
      "outputs": [],
      "source": [
        "#@title Clone Repo & Install Requirements { display-mode: \"form\" }\n",
        "%%capture\n",
        "\n",
        "# Clone Repo\n",
        "%cd /content\n",
        "!git clone https://github.com/mhsotoudeh/ProbUNet-Tutorial.git\n",
        "# !export PYTHONPATH=\"${PYTHONPATH}:$PWD/ProbUNet-Tutorial\"\n",
        "%cd /content/ProbUNet-Tutorial\n",
        "\n",
        "# Install Requirements\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwPgJQeCKLkI"
      },
      "outputs": [],
      "source": [
        "#@title Imports { display-mode: \"form\" }\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "from model import *\n",
        "from train import *\n",
        "\n",
        "from random import randrange\n",
        "\n",
        "from dotmap import DotMap\n",
        "from IPython import display\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99sxH0ZRKLkI"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device is {}\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpSSNgMpKLkJ"
      },
      "source": [
        "# Part 2: Rescue the Randomness (Loss Functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Functions"
      ],
      "metadata": {
        "id": "Hqi5qnWE2XGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialization { display-mode: \"form\" }\n",
        "# Initialization\n",
        "\n",
        "def initialize(args):\n",
        "    # Set Random Seed\n",
        "    np.random.seed(args.random_seed)\n",
        "    torch.manual_seed(args.random_seed)\n",
        "\n",
        "    # Initialize Model\n",
        "    model = HPUNet( in_ch=args.in_ch, out_ch=args.out_ch, chs=args.intermediate_ch,\n",
        "                    latent_num=args.latent_num, latent_channels=args.latent_chs, latent_locks=args.latent_locks,\n",
        "                    scale_depth=args.scale_depth, kernel_size=args.kernel_size, dilation=args.dilation,\n",
        "                    padding_mode=args.padding_mode, conv_dim=1 )\n",
        "\n",
        "    args.trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    # Set Loss Function\n",
        "\n",
        "    ## Reconstruction Loss\n",
        "    if args.rec_type.lower() == 'mse':\n",
        "        reconstruction_loss = MSELossWrapper()\n",
        "\n",
        "    else:\n",
        "        print('Invalid reconstruction loss type, exiting...')\n",
        "        exit()\n",
        "\n",
        "\n",
        "    ## Total Loss\n",
        "    if args.loss_type.lower() == 'elbo':\n",
        "        if args.beta_asc_steps is None:\n",
        "            beta_scheduler = BetaConstant(args.beta)\n",
        "        else:\n",
        "            beta_scheduler = BetaLinearScheduler(ascending_steps=args.beta_asc_steps, constant_steps=args.beta_cons_steps, max_beta=args.beta, saturation_step=args.beta_saturation_step)\n",
        "        criterion = ELBOLoss(reconstruction_loss=reconstruction_loss, beta=beta_scheduler).to(device)\n",
        "\n",
        "    elif args.loss_type.lower() == 'geco':\n",
        "        kappa = args.kappa\n",
        "        if args.kappa_px is True:\n",
        "            kappa *= n\n",
        "        criterion = GECOLoss(reconstruction_loss=reconstruction_loss, kappa=kappa, decay=args.decay, update_rate=args.update_rate, device=device).to(device)\n",
        "\n",
        "    else:\n",
        "        print('Invalid loss type, exiting...')\n",
        "        exit()\n",
        "\n",
        "\n",
        "    # Set Optimizer\n",
        "    if args.optimizer == 'adamax':\n",
        "        optimizer = optim.Adamax(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "\n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "\n",
        "    elif args.optimizer == 'adamw':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "\n",
        "    else:\n",
        "        print('Optimizer not known, exiting...')\n",
        "        exit()\n",
        "\n",
        "\n",
        "    # Set LR Scheduler\n",
        "    if args.scheduler_type == 'cons':\n",
        "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.epochs)\n",
        "\n",
        "    elif args.scheduler_type == 'step':\n",
        "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.scheduler_step_size, gamma=args.scheduler_gamma)\n",
        "\n",
        "    elif args.scheduler_type == 'milestones':\n",
        "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.scheduler_milestones, gamma=args.scheduler_gamma)\n",
        "\n",
        "    return model, criterion, optimizer, lr_scheduler"
      ],
      "metadata": {
        "id": "2PcNGTc22WeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training { display-mode: \"form\" }\n",
        "# Training\n",
        "\n",
        "def record_history(idx, loss_dict, type='train'):\n",
        "    prefix = 'Minibatch Training ' if type == 'train' else 'Mean Validation '\n",
        "\n",
        "    loss_per_pixel = loss_dict['loss'].item() / args.pixels\n",
        "    reconstruction_per_pixel = loss_dict['reconstruction_term'].item() / args.pixels\n",
        "    kl_term_per_pixel = loss_dict['kl_term'].item() / args.pixels\n",
        "    kl_per_pixel = [ loss_dict['kls'][v].item() / args.pixels for v in range(args.latent_num) ]\n",
        "\n",
        "    # Total Loss\n",
        "    _dict = {   'total': loss_per_pixel,\n",
        "                'kl term': kl_term_per_pixel, \n",
        "                'reconstruction': reconstruction_per_pixel  }\n",
        "    writer.add_scalars(prefix + 'Loss Curve', _dict, idx)\n",
        "\n",
        "    # KL Term Decomposition\n",
        "    _dict = { 'sum': sum(kl_per_pixel) }\n",
        "    _dict.update( { 'scale {}'.format(v): kl_per_pixel[v] for v in range(args.latent_num) } )\n",
        "    writer.add_scalars(prefix + 'Loss Curve (K-L)', _dict, idx)\n",
        "\n",
        "    # Coefficients\n",
        "    if type == 'train':\n",
        "        if args.loss_type.lower() == 'elbo':\n",
        "            writer.add_scalar('Beta', criterion.beta_scheduler.beta, idx)\n",
        "        elif args.loss_type.lower() == 'geco':\n",
        "            lamda = criterion.log_inv_function(criterion.log_lamda).item()\n",
        "            writer.add_scalar('Lagrange Multiplier', lamda, idx)\n",
        "            writer.add_scalar('Beta', 1/(lamda+1e-20), idx)\n",
        "\n",
        "\n",
        "def train():\n",
        "    for e in tqdm_notebook(range(args.epochs)):\n",
        "        # Initialization\n",
        "        criterion.train()\n",
        "        model.train()\n",
        "        model.zero_grad()\n",
        "        \n",
        "        # Train One Step\n",
        "        \n",
        "        ## Generate Truths\n",
        "        p = torch.randperm(32, device=device)           # Generate a random permutation to permute training data at each iteration\n",
        "        noise = torch.randn(n,1,n, device=device)*sigma\n",
        "        unrolled_truths = raw_truths + noise\n",
        "        truths = random_roll(unrolled_truths)[p]\n",
        "        \n",
        "        ## Get Predictions and Prepare for Loss Calculation\n",
        "        if args.rec_type.lower() == 'mse':\n",
        "            preds, infodicts = model(inputs[p], truths)\n",
        "            preds, infodict = preds[:,0], infodicts[0]\n",
        "\n",
        "        truths = truths.squeeze(dim=1)\n",
        "        \n",
        "        \n",
        "        ## Calculate Loss\n",
        "        loss = criterion(preds, truths, kls=infodict['kls'], lr=lr_scheduler.get_last_lr()[0])\n",
        "\n",
        "\n",
        "        ## Backpropagate\n",
        "        loss.backward()             # Calculate Gradients\n",
        "        optimizer.step()            # Update Weights\n",
        "\n",
        "\n",
        "        ## Step Beta Scheduler\n",
        "        if args.loss_type.lower() == 'elbo':\n",
        "            criterion.beta_scheduler.step()\n",
        "\n",
        "\n",
        "        # Record Train History\n",
        "        loss_dict = criterion.last_loss.copy()\n",
        "        loss_dict.update( { 'kls': infodict['kls'] } )\n",
        "\n",
        "        record_history(e, loss_dict)\n",
        "\n",
        "\n",
        "        # Validation\n",
        "        if (e+1) % args.val_period == 0:\n",
        "            criterion.eval()\n",
        "            model.eval()\n",
        "\n",
        "            noise = torch.randn(n,1,n, device=device)*sigma\n",
        "            unrolled_truths = raw_truths + noise\n",
        "            truths = random_roll(unrolled_truths)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                preds, infodicts = model(inputs, truths, times=args.k, insert_from_postnet=False)\n",
        "\n",
        "            fig = plot_latents_or_pca(infodicts, scale=-1, step=e+1)\n",
        "            # fig.tight_layout()\n",
        "            writer.add_figure('Evolution of Latents', fig, e+1)\n",
        "            \n",
        "\n",
        "    # Save Model & Loss After Training is Done\n",
        "    torch.save(model, 'runs/part2/{}/model.pth'.format(stamp))\n",
        "    torch.save(criterion, 'runs/part2/{}/loss.pth'.format(stamp))"
      ],
      "metadata": {
        "id": "iA-c_mNH2hEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluation { display-mode: \"form\" }\n",
        "# Evaluation\n",
        "\n",
        "# colors = ['#ff0000', '#ff3100', '#ff6100', '#ff9200', '#ffc200', '#fef200', '#daff00', '#aaff00', '#79ff00', '#49ff00', '#18ff00', '#00ff18', '#00ff49', '#00ff7a', '#00ffaa', '#00ffdb', '#00f3ff', '#00c2ff', '#0091ff', '#0061ff', '#0030ff', '#0505ff', '#3100ff', '#6100ff', '#9200ff', '#c300ff', '#f300ff', '#ff00da', '#ff00aa', '#ff0079', '#ff0048', '#ff0018']\n",
        "\n",
        "# function to sample n colors from the hsv colormap (to assign each training example a color)\n",
        "def get_colors(n):\n",
        "    cmap = cm.get_cmap('hsv', n)\n",
        "    color_list = [mpl.colors.rgb2hex(cmap(i)[:3]) for i in range(cmap.N)]\n",
        "\n",
        "    return color_list\n",
        "\n",
        "\n",
        "# function to plot components (dim = 2) or two given components (dim > 2) of the latent space at a given scale against each other\n",
        "def plot_latents(infodicts, scale, step, comp_pair=None, show=False):\n",
        "    k = len(infodicts)\n",
        "\n",
        "    prior_latents, post_latents = [], []\n",
        "    for i in range(k):\n",
        "        prior_latents.append( infodicts[i]['prior_latents'][scale].squeeze().cpu().numpy() )\n",
        "        post_latents.append( infodicts[i]['post_latents'][scale].squeeze().cpu().numpy() )\n",
        "\n",
        "    prior_latents, post_latents = np.stack(prior_latents), np.stack(post_latents)\n",
        "    \n",
        "    dim = prior_latents.shape[-1]\n",
        "    assert dim >= 2\n",
        "    if dim > 2:  # if dim > 2, the two components given by comp_pair argument will be plotted\n",
        "        assert comp_pair is not None\n",
        "        assert len(comp_pair) == 2\n",
        "        prior_latents = prior_latents[:,:,comp_pair]\n",
        "        post_latents = post_latents[:,:,comp_pair]\n",
        "\n",
        "    # Create a figure with two subplots (one for PriorNet latents and another for PosteriorNet latents)\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
        "\n",
        "    # Set the titles for the figure and each subplot\n",
        "    fig.suptitle('Latent Space Samples of Scale {} at Step {} (dim = {})'.format(scale, step, dim) + (' / Components: {}'.format(comp_pair) if dim > 2 else ''), size=14)\n",
        "    axs[0].set_title('PriorNet Latents')\n",
        "    axs[1].set_title('PoteriorNet Latents')\n",
        "\n",
        "    # For each subplot, turn off the axes\n",
        "    [axi.set_axis_off() for axi in axs.ravel()]\n",
        "\n",
        "    for i in range(k):\n",
        "        axs[0].scatter(prior_latents[i,:,0], prior_latents[i,:,1], c=colors, alpha=0.7, s=20)\n",
        "        axs[1].scatter(post_latents[i,:,0], post_latents[i,:,1], c=colors, alpha=0.7, s=20)\n",
        "\n",
        "    # If the show parameter is True, display the plot\n",
        "    if show is True:\n",
        "        plt.show()\n",
        "\n",
        "    # Close the figure and return it\n",
        "    plt.close()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# function to plot components (dim = 2) or the first two principal components (dim > 2) of the latent space at a given scale against each other\n",
        "def plot_latents_or_pca(infodicts, scale, step, show=False):\n",
        "    k = len(infodicts)\n",
        "\n",
        "    prior_latents, post_latents = [], []\n",
        "    for i in range(k):\n",
        "        prior_latents.append( infodicts[i]['prior_latents'][scale].squeeze().cpu().numpy() )\n",
        "        post_latents.append( infodicts[i]['post_latents'][scale].squeeze().cpu().numpy() )\n",
        "\n",
        "    prior_latents, post_latents = np.stack(prior_latents), np.stack(post_latents)\n",
        "    \n",
        "    dim = prior_latents.shape[-1]\n",
        "    assert dim >= 2\n",
        "    if dim > 2:  # if dim > 2, the first two principal components will be plotted\n",
        "        pca = PCA(n_components=2)\n",
        "        prior_latents = pca.fit_transform(prior_latents.reshape(-1,dim)).reshape(k,n,2)\n",
        "        post_latents = pca.fit_transform(post_latents.reshape(-1,dim)).reshape(k,n,2)\n",
        "\n",
        "    # Create a figure with two subplots (one for PriorNet latents and another for PosteriorNet latents)\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
        "\n",
        "    # Set the titles for the figure and each subplot\n",
        "    fig.suptitle('Latent Space Samples of Scale {} at Step {} (dim = {})'.format(scale, step, dim) + (' / First Two PCs' if dim > 2 else ''), size=14)\n",
        "    axs[0].set_title('PriorNet Latents')\n",
        "    axs[1].set_title('PoteriorNet Latents')\n",
        "\n",
        "    # For each subplot, turn off the axes\n",
        "    [axi.set_axis_off() for axi in axs.ravel()]\n",
        "\n",
        "    for i in range(k):\n",
        "        axs[0].scatter(prior_latents[i,:,0], prior_latents[i,:,1], c=colors, alpha=0.7, s=20)\n",
        "        axs[1].scatter(post_latents[i,:,0], post_latents[i,:,1], c=colors, alpha=0.7, s=20)\n",
        "\n",
        "    # If the show parameter is True, display the plot\n",
        "    if show is True:\n",
        "        plt.show()\n",
        "\n",
        "    # Close the figure and return it\n",
        "    plt.close()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# function to plot a given prediction (idx) of a given example (ex) in the dataset\n",
        "def plot_sample(idx, inputs, truths, preds, bounds=None, common_pred_colormap=True, show=False):\n",
        "    \n",
        "    # Concatenate the observations, ground truth, and predictions into a single tensor (to calculate common color axis limits)\n",
        "    _all = torch.cat([inputs, truths, preds], dim=1)\n",
        "    \n",
        "    # Convert torch tensors to numpy arrays\n",
        "    inputs = inputs.squeeze().cpu().numpy()\n",
        "    truths = truths.squeeze().cpu().numpy()\n",
        "    preds = preds.cpu().numpy()\n",
        "\n",
        "    # Determine color axis limits\n",
        "    if bounds is None:\n",
        "        _min, _max = _all.min(), _all.max()\n",
        "    else:\n",
        "        _min, _max = bounds\n",
        "    pred_min, pred_max = _min if common_pred_colormap is True else preds.min(), _max if common_pred_colormap is True else preds.max()\n",
        "    \n",
        "    # Create a figure with five subplots\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(25,5))\n",
        "\n",
        "    # Set the titles for each subplot\n",
        "    # fig.suptitle('Training Dataset (each row is a training example)', size=14)\n",
        "    axs[0].set_title('Inputs')\n",
        "    axs[1].set_title('Ground Truths')\n",
        "    axs[2].set_title('Outputs {}'.format(idx+1))\n",
        "    axs[3].set_title('Means')\n",
        "    axs[4].set_title('STDs')\n",
        "\n",
        "    # Display the observation, ground truth, prediction, mean and std maps\n",
        "    im0 = axs[0].imshow(inputs, vmin=_min, vmax=_max)\n",
        "    im1 = axs[1].imshow(truths, vmin=_min, vmax=_max)\n",
        "    im2 = axs[2].imshow(preds[:,idx], vmin=pred_min, vmax=pred_max)\n",
        "    im3 = axs[3].imshow(preds.mean(axis=1), vmin=pred_min, vmax=pred_max)\n",
        "    im4 = axs[4].imshow(preds.std(axis=1), cmap='magma')\n",
        "\n",
        "    # Create a list of image objects to be used for color bar display\n",
        "    imlist = [im0, im1, im2, im3, im4]\n",
        "    \n",
        "    # For each subplot, turn off the axes, create a new axis for the color bar, and add it to the figure\n",
        "    for i, axi in enumerate(axs.ravel()):\n",
        "        axi.set_axis_off()\n",
        "\n",
        "        divider = make_axes_locatable(axi)\n",
        "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "        fig.colorbar(imlist[i], cax=cax, orientation='vertical')\n",
        "\n",
        "        imlist.append(cax)\n",
        "\n",
        "    # If the show parameter is True, display the plot\n",
        "    if show is True:\n",
        "        plt.show()\n",
        "\n",
        "    # Close the figure and return it\n",
        "    plt.close()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# function to creat an animation of \"num\" predictions of a given example (ex) in the dataset\n",
        "def animate_samples(inputs, truths, preds, bounds=None, common_pred_colormap=True, num=None, output_type='jshtml'):\n",
        "    # Make sure the number of predictions to display is less than or equal to the total number of available predictions\n",
        "    if num is not None:\n",
        "        assert num <= preds.shape[1]\n",
        "    \n",
        "    # Concatenate the observations, ground truth, and predictions into a single tensor (to calculate common color axis limits)\n",
        "    _all = torch.cat([inputs, truths, preds], dim=1)\n",
        "    \n",
        "    # Convert torch tensors to numpy arrays\n",
        "    inputs = inputs.squeeze().cpu().numpy()\n",
        "    truths = truths.squeeze().cpu().numpy()\n",
        "    preds = preds.cpu().numpy()\n",
        "\n",
        "    # Determine color axis limits\n",
        "    if bounds is None:\n",
        "        _min, _max = _all.min(), _all.max()\n",
        "    else:\n",
        "        _min, _max = bounds\n",
        "    pred_min, pred_max = _min if common_pred_colormap is True else preds.min(), _max if common_pred_colormap is True else preds.max()\n",
        "    \n",
        "    # Create a figure with five subplots\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(21.5,4.3))\n",
        "\n",
        "    # Set the titles for each subplot\n",
        "    # fig.suptitle('Training Dataset (each row is a training example)', size=14)\n",
        "    axs[0].set_title('Inputs')\n",
        "    axs[1].set_title('Ground Truths')\n",
        "    axs[2].set_title('Outputs 1')\n",
        "    axs[3].set_title('Means')\n",
        "    axs[4].set_title('STDs')\n",
        "\n",
        "    # Display the observation, ground truth, prediction, mean and std maps\n",
        "    im0 = axs[0].imshow(inputs, vmin=_min, vmax=_max)\n",
        "    im1 = axs[1].imshow(truths, vmin=_min, vmax=_max)\n",
        "    im2 = axs[2].imshow(preds[:,0], vmin=pred_min, vmax=pred_max)\n",
        "    im3 = axs[3].imshow(preds.mean(axis=1), vmin=pred_min, vmax=pred_max)\n",
        "    im4 = axs[4].imshow(preds.std(axis=1), cmap='magma')\n",
        "\n",
        "    # Create a list of image objects to be used for color bar display\n",
        "    imlist = [im0, im1, im2, im3, im4]\n",
        "    \n",
        "    # For each subplot, turn off the axes, create a new axis for the color bar, and add it to the figure\n",
        "    for i, axi in enumerate(axs.ravel()):\n",
        "        axi.set_axis_off()\n",
        "\n",
        "        divider = make_axes_locatable(axi)\n",
        "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "        fig.colorbar(imlist[i], cax=cax, orientation='vertical')\n",
        "\n",
        "        imlist.append(cax)\n",
        "\n",
        "    # Function to update the prediction subplot for each frame of the animation\n",
        "    def animate(i):\n",
        "        axs[2].set_title('Outputs {}'.format(i+1))\n",
        "        im2 = axs[2].imshow(preds[:,i], vmin=pred_min, vmax=pred_max, animated=True) \n",
        "\n",
        "        return im2,\n",
        "\n",
        "    # Set the total number of frames\n",
        "    frms = num if num is not None else preds.shape[1]    \n",
        "    \n",
        "    # Set the padding of the plot\n",
        "    plt.tight_layout(pad=2)\n",
        "    \n",
        "    # Generate animation frames\n",
        "    anim = animation.FuncAnimation(fig, animate, frames=frms, interval=100, blit=True, repeat_delay=1000)\n",
        "    \n",
        "    # Close the figure\n",
        "    plt.close()\n",
        "\n",
        "    # Genrate an return the animation output\n",
        "    if output_type == 'video':\n",
        "        out = anim.to_html5_video()\n",
        "    elif output_type == 'jshtml':\n",
        "        out = anim.to_jshtml()\n",
        "\n",
        "    html = display.HTML(out)\n",
        "    return html"
      ],
      "metadata": {
        "id": "GGuNrOZV2hIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvwAh5vnKLkJ"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_roll(a):\n",
        "    n = a.shape[0]\n",
        "\n",
        "    r = torch.rand(n)\n",
        "    rolls = torch.zeros_like(r, dtype=torch.int8)\n",
        "    rolls[r < 0.1] = -2\n",
        "    rolls[(0.1 < r) & (r < 0.3)] = -1\n",
        "    rolls[(0.3 < r) & (r < 0.7)] = 0\n",
        "    rolls[(0.7 < r) & (r < 0.9)] = 1\n",
        "    rolls[0.9 < r] = 2\n",
        "\n",
        "    output = list(map(torch.roll, torch.unbind(a, dim=0), rolls.numpy()))\n",
        "    return torch.stack(output, dim=0)"
      ],
      "metadata": {
        "id": "7l4ZOV8jWi5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXDeJ_LUKLkJ"
      },
      "outputs": [],
      "source": [
        "# Dataset Parameters\n",
        "\n",
        "n = 32                        # Dataset Size\n",
        "sigma = 0.1                   # Noise Level\n",
        "\n",
        "# assign the color of each training example\n",
        "colors = get_colors(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHXNrwe-KLkJ"
      },
      "outputs": [],
      "source": [
        "# Create the Dataset\n",
        "\n",
        "a = torch.arange(n, device=device)\n",
        "\n",
        "inputs = torch.zeros(n,n, device=device)\n",
        "inputs[torch.arange(n), a] = 1\n",
        "\n",
        "raw_truths = torch.flip(inputs, dims=(1,))\n",
        "\n",
        "inputs, raw_truths = inputs.unsqueeze(dim=1), raw_truths.unsqueeze(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3FldXfVKLkK"
      },
      "outputs": [],
      "source": [
        "# Generate a Realization of Truths\n",
        "\n",
        "noise = torch.randn(n,1,n, device=device)*sigma\n",
        "unrolled_truths = raw_truths + noise\n",
        "truths = random_roll(unrolled_truths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YrBCEagKLkK"
      },
      "outputs": [],
      "source": [
        "# Visualize the Dataset\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(24,6))\n",
        "\n",
        "fig.suptitle('Training Dataset (each row is a training example)', size=14)\n",
        "\n",
        "axs[0].set_title('Input')\n",
        "axs[1].set_title('Raw Truth (without noise and unrolled)')\n",
        "axs[2].set_title('Unrolled Truth (noise added)')\n",
        "axs[3].set_title('Truth')\n",
        "\n",
        "axs[0].imshow(inputs.squeeze().cpu().numpy())\n",
        "axs[1].imshow(raw_truths.squeeze().cpu().numpy())\n",
        "axs[2].imshow(unrolled_truths.squeeze().cpu().numpy())\n",
        "axs[3].imshow(truths.squeeze().cpu().numpy())\n",
        "\n",
        "[axi.set_axis_off() for axi in axs.ravel()]\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CEwYM2YKLkK"
      },
      "outputs": [],
      "source": [
        "inputs.shape, raw_truths.shape, unrolled_truths.shape, truths.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmoRLCGnKLkL"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constant Args\n",
        "args = DotMap()\n",
        "\n",
        "args.random_seed = 42\n",
        "\n",
        "## Data\n",
        "args.pixels = n\n",
        "\n",
        "## Model\n",
        "args.in_ch, args.out_ch = 1, 1\n",
        "args.intermediate_ch = [4, 8, 8, 16, 16]\n",
        "args.kernel_size = [3, 3, 3, 3, 3]\n",
        "args.scale_depth, args.dilation = [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]\n",
        "args.padding_mode = 'zeros' \n",
        "\n",
        "args.latent_num = 5\n",
        "args.latent_chs = [1, 1, 1, 1, 1]\n",
        "args.latent_locks = [False, False, False, False, False]\n",
        "\n",
        "## Training\n",
        "args.epochs = 8000\n",
        "args.optimizer = 'adamax'\n",
        "args.wd = 1e-5\n",
        "args.lr = 5e-4\n",
        "args.scheduler_type = 'cons'\n",
        "\n",
        "## Validation\n",
        "args.val_period = 1000\n",
        "args.k = 100"
      ],
      "metadata": {
        "id": "4ZDvcVPcIA05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard Session"
      ],
      "metadata": {
        "id": "g4LxQ5fHJvKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs/part2"
      ],
      "metadata": {
        "id": "9qTRJlX6JvWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "AiNtQ0eeJOAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ELBO** - Constant Beta ($\\beta = 1$)"
      ],
      "metadata": {
        "id": "4oCVBFxfImPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stamp\n",
        "stamp = 'elbo / beta = 1'\n",
        "\n",
        "# Initialize SummaryWriter (for tensorboard)\n",
        "writer = SummaryWriter('runs/part2/{}/tb'.format(stamp))\n",
        "\n",
        "# Variable Args\n",
        "## Loss\n",
        "args.rec_type = 'mse'\n",
        "args.loss_type = 'elbo'\n",
        "args.beta = 1.0\n",
        "args.beta_asc_steps = None"
      ],
      "metadata": {
        "id": "UiY3TGfjrhqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and Train the Model\n",
        "\n",
        "model, criterion, optimizer, lr_scheduler = initialize(args)\n",
        "train()"
      ],
      "metadata": {
        "id": "6WI-aDs-rhtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Samples for Evaluation\n",
        "\n",
        "k = 100                        # Num of samples per training example\n",
        "\n",
        "model.eval()\n",
        "criterion.eval()\n",
        "\n",
        "noise = torch.randn(n,1,n, device=device)*sigma\n",
        "unrolled_truths = raw_truths + noise\n",
        "truths = random_roll(unrolled_truths)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds, infodicts = model(inputs, truths, times=k, insert_from_postnet=False)"
      ],
      "metadata": {
        "id": "icpg_F2BW9FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot or Animate Samples\n",
        "\n",
        "# plot_sample(80, inputs, truths, preds)\n",
        "\n",
        "html = animate_samples(inputs, truths, preds, bounds=(-0.2,1.2), num=30)\n",
        "display.display(html)"
      ],
      "metadata": {
        "id": "0SGo2OfiW9I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Latent Representations\n",
        "\n",
        "plot_latents_or_pca(infodicts, scale=4, step=args.epochs)"
      ],
      "metadata": {
        "id": "m0w66upukuX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ELBO** - Constant Beta ($\\beta = 0.7$)"
      ],
      "metadata": {
        "id": "piDM1JesFzrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stamp\n",
        "stamp = 'elbo / beta = 0.7'\n",
        "\n",
        "# Initialize SummaryWriter (for tensorboard)\n",
        "writer = SummaryWriter('runs/part2/{}/tb'.format(stamp))\n",
        "\n",
        "# Variable Args\n",
        "## Loss\n",
        "args.rec_type = 'mse'\n",
        "args.loss_type = 'elbo'\n",
        "args.beta = 0.1\n",
        "args.beta_asc_steps = None"
      ],
      "metadata": {
        "id": "aqop1YYDFvI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and Train the Model\n",
        "\n",
        "model, criterion, optimizer, lr_scheduler = initialize(args)\n",
        "train()"
      ],
      "metadata": {
        "id": "XUMDokH1F-T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Samples for Evaluation\n",
        "\n",
        "k = 100                        # Num of samples per training example\n",
        "\n",
        "model.eval()\n",
        "criterion.eval()\n",
        "\n",
        "noise = torch.randn(n,1,n, device=device)*sigma\n",
        "unrolled_truths = raw_truths + noise\n",
        "truths = random_roll(unrolled_truths)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds, infodicts = model(inputs, truths, times=k, insert_from_postnet=False)"
      ],
      "metadata": {
        "id": "YX4-DS6ZF-WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot or Animate Samples\n",
        "\n",
        "# plot_sample(80, inputs, truths, preds)\n",
        "\n",
        "html = animate_samples(inputs, truths, preds, bounds=(-0.2,1.2), num=30)\n",
        "display.display(html)"
      ],
      "metadata": {
        "id": "ajjvEXM3F-Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Latent Representations\n",
        "\n",
        "plot_latents_or_pca(infodicts, scale=4, step=args.epochs)"
      ],
      "metadata": {
        "id": "iVVBILxZGHHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ELBO** - Linear Beta"
      ],
      "metadata": {
        "id": "pUpqo4KuLMUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stamp\n",
        "stamp = 'elbo / linear beta'\n",
        "\n",
        "# Initialize SummaryWriter (for tensorboard)\n",
        "writer = SummaryWriter('runs/part2/{}/tb'.format(stamp))\n",
        "\n",
        "# Variable Args\n",
        "## Loss\n",
        "args.rec_type = 'mse'\n",
        "args.loss_type = 'elbo'\n",
        "args.beta = 1.0\n",
        "args.beta_asc_steps = 8000\n",
        "args.beta_cons_steps = None\n",
        "args.beta_saturation_step = 8000"
      ],
      "metadata": {
        "id": "5dG_l_5WW9LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and Train the Model\n",
        "\n",
        "model, criterion, optimizer, lr_scheduler = initialize(args)\n",
        "train()"
      ],
      "metadata": {
        "id": "0RRXmws2ApOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Samples for Evaluation\n",
        "\n",
        "k = 100                        # Num of samples per training example\n",
        "\n",
        "model.eval()\n",
        "criterion.eval()\n",
        "\n",
        "noise = torch.randn(n,1,n, device=device)*sigma\n",
        "unrolled_truths = raw_truths + noise\n",
        "truths = random_roll(unrolled_truths)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds, infodicts = model(inputs, truths, times=k, insert_from_postnet=False)"
      ],
      "metadata": {
        "id": "BMri-EADApRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot or Animate Samples\n",
        "\n",
        "# plot_sample(80, inputs, truths, preds)\n",
        "\n",
        "html = animate_samples(inputs, truths, preds, bounds=(-0.2,1.2), num=30)\n",
        "display.display(html)"
      ],
      "metadata": {
        "id": "H5U6h6KGApqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Latent Representations\n",
        "\n",
        "plot_latents_or_pca(infodicts, scale=4, step=args.epochs)"
      ],
      "metadata": {
        "id": "elpaZtLDApnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ELBO** - Cyclical Beta"
      ],
      "metadata": {
        "id": "0BN1g9BmKpj9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFXxtcc8KLkP"
      },
      "outputs": [],
      "source": [
        "# Stamp\n",
        "stamp = 'elbo / cyclical beta'\n",
        "\n",
        "# Initialize SummaryWriter (for tensorboard)\n",
        "writer = SummaryWriter('runs/part2/{}/tb'.format(stamp))\n",
        "\n",
        "# Variable Args\n",
        "## Loss\n",
        "args.rec_type = 'mse'\n",
        "args.loss_type = 'elbo'\n",
        "args.beta = 1.0\n",
        "args.beta_asc_steps = 100\n",
        "args.beta_cons_steps = 100\n",
        "args.beta_saturation_step = args.epochs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and Train the Model\n",
        "\n",
        "model, criterion, optimizer, lr_scheduler = initialize(args)\n",
        "train()"
      ],
      "metadata": {
        "id": "NhKZ2_YsQtqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Samples for Evaluation\n",
        "\n",
        "k = 100                        # Num of samples per training example\n",
        "\n",
        "model.eval()\n",
        "criterion.eval()\n",
        "\n",
        "noise = torch.randn(n,1,n, device=device)*sigma\n",
        "unrolled_truths = raw_truths + noise\n",
        "truths = random_roll(unrolled_truths)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds, infodicts = model(inputs, truths, times=k, insert_from_postnet=False)"
      ],
      "metadata": {
        "id": "H3VCi-o_LSz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot or Animate Samples\n",
        "\n",
        "# plot_sample(80, inputs, truths, preds)\n",
        "\n",
        "html = animate_samples(inputs, truths, preds, bounds=(-0.2,1.2), num=30)\n",
        "display.display(html)"
      ],
      "metadata": {
        "id": "-CKwZ7l0TRW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Latent Representations\n",
        "\n",
        "plot_latents_or_pca(infodicts, scale=4, step=args.epochs)"
      ],
      "metadata": {
        "id": "eyUZq6BfTRUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ELBO** - Constant Beta ($\\beta = 0$)"
      ],
      "metadata": {
        "id": "QJdv-lDJLTp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stamp\n",
        "stamp = 'elbo / beta = 0'\n",
        "\n",
        "# Initialize SummaryWriter (for tensorboard)\n",
        "writer = SummaryWriter('runs/part2/{}/tb'.format(stamp))\n",
        "\n",
        "# Variable Args\n",
        "## Loss\n",
        "args.rec_type = 'mse'\n",
        "args.loss_type = 'elbo'\n",
        "args.beta = 0.0\n",
        "args.beta_asc_steps = None"
      ],
      "metadata": {
        "id": "y48Vsk-FLS2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and Train the Model\n",
        "\n",
        "model, criterion, optimizer, lr_scheduler = initialize(args)\n",
        "train()"
      ],
      "metadata": {
        "id": "aEMYB_DBA7ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Samples for Evaluation\n",
        "\n",
        "k = 100                        # Num of samples per training example\n",
        "\n",
        "model.eval()\n",
        "criterion.eval()\n",
        "\n",
        "noise = torch.randn(n,1,n, device=device)*sigma\n",
        "unrolled_truths = raw_truths + noise\n",
        "truths = random_roll(unrolled_truths)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds, infodicts = model(inputs, truths, times=k, insert_from_postnet=False)"
      ],
      "metadata": {
        "id": "EmlYcE3OLS6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot or Animate Samples\n",
        "\n",
        "# plot_sample(80, inputs, truths, preds)\n",
        "\n",
        "html = animate_samples(inputs, truths, preds, bounds=(-0.2,1.2), num=30)\n",
        "display.display(html)"
      ],
      "metadata": {
        "id": "zH79bhXDLaXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Latent Representations\n",
        "\n",
        "plot_latents_or_pca(infodicts, scale=4, step=args.epochs)"
      ],
      "metadata": {
        "id": "MXIqWSXTLabN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **GECO**"
      ],
      "metadata": {
        "id": "20EIz-YBLatY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stamp\n",
        "stamp = 'geco / kappa 0.01'\n",
        "\n",
        "# Initialize SummaryWriter (for tensorboard)\n",
        "writer = SummaryWriter('runs/part2/{}/tb'.format(stamp))\n",
        "\n",
        "# Variable Args\n",
        "## Loss\n",
        "args.rec_type = 'mse'\n",
        "args.loss_type = 'geco'\n",
        "args.kappa = 0.01\n",
        "args.kappa_px = True\n",
        "args.decay = 0.9\n",
        "args.update_rate = 1.0"
      ],
      "metadata": {
        "id": "tklTpn7CLeH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and Train the Model\n",
        "\n",
        "model, criterion, optimizer, lr_scheduler = initialize(args)\n",
        "train()"
      ],
      "metadata": {
        "id": "tVNEYJ5lA84c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Samples for Evaluation\n",
        "\n",
        "k = 100                        # Num of samples per training example\n",
        "\n",
        "model.eval()\n",
        "criterion.eval()\n",
        "\n",
        "noise = torch.randn(n,1,n, device=device)*sigma\n",
        "unrolled_truths = raw_truths + noise\n",
        "truths = random_roll(unrolled_truths)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds, infodicts = model(inputs, truths, times=k, insert_from_postnet=False)"
      ],
      "metadata": {
        "id": "3avRCX7PLe0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot or Animate Samples\n",
        "\n",
        "# plot_sample(80, inputs, truths, preds)\n",
        "\n",
        "html = animate_samples(inputs, truths, preds, bounds=(-0.2,1.2), num=30)\n",
        "display.display(html)"
      ],
      "metadata": {
        "id": "JtHdXs6n3QpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Latent Representations\n",
        "\n",
        "plot_latents_or_pca(infodicts, scale=4, step=args.epochs)"
      ],
      "metadata": {
        "id": "5vVelyzZzIxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6j3Xs1Iz6x0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4oCVBFxfImPW",
        "piDM1JesFzrI",
        "pUpqo4KuLMUF",
        "0BN1g9BmKpj9",
        "QJdv-lDJLTp-",
        "20EIz-YBLatY"
      ]
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}